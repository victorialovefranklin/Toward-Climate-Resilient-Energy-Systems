{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsOmuaxXFS++03WL63npA5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorialovefranklin/Toward-Climate-Resilient-Energy-Systems/blob/main/EAGLE_I_Data_Integration_Cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EAGLE-I (2014-2023) Data Integration & Cleaning**"
      ],
      "metadata": {
        "id": "KBvlHtwFcdcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task:** Load the 10 files Environment for Analysis of Geo-Located Energy Information (EAGLE-I) datasets across 2014 to 2023. Export clean files with seperationof State fips and county fips columns."
      ],
      "metadata": {
        "id": "dId64_MrWPcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# EAGLE-I (2014–2023) — Data Integration & Cleaning\n",
        "# ==============================\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# -------- Explicit file list (each year, 2014–2023) --------\n",
        "YEAR_TO_PATH = {\n",
        "    2014: \"/content/eaglei_outages_2014.csv\",\n",
        "    2015: \"/content/eaglei_outages_2015.csv\",\n",
        "    2016: \"/content/eaglei_outages_2016.csv\",\n",
        "    2017: \"/content/eaglei_outages_2017.csv\",\n",
        "    2018: \"/content/eaglei_outages_2018.csv\",\n",
        "    2019: \"/content/eaglei_outages_2019.csv\",\n",
        "    2020: \"/content/eaglei_outages_2020.csv\",\n",
        "    2021: \"/content/eaglei_outages_2021.csv\",\n",
        "    2022: \"/content/eaglei_outages_2022.csv\",\n",
        "    2023: \"/content/eaglei_outages_2023.csv\",\n",
        "}\n",
        "\n",
        "# CLEANED_OUTPUT_FILE = \"/content/eaglei_outages_cleaned.csv\" # Removed merged output file\n",
        "\n",
        "print(\"Planned files for analysis (2014–2023):\")\n",
        "for y, p in YEAR_TO_PATH.items():\n",
        "    print(f\" - {y}: {p}\")\n",
        "\n",
        "# -------- Process, Clean, and Export Data Per Year --------\n",
        "file_report = []   # to summarize loads per file\n",
        "missing_files = []\n",
        "total_original_rows = 0\n",
        "total_cleaned_rows = 0\n",
        "key_columns_used = set()\n",
        "\n",
        "\n",
        "for yr, fp in YEAR_TO_PATH.items():\n",
        "    if os.path.exists(fp):\n",
        "        try:\n",
        "            df_i = pd.read_csv(fp, low_memory=False)\n",
        "            original_rows = len(df_i) # Store original row count\n",
        "            total_original_rows += original_rows\n",
        "            # force year column to exist and be int\n",
        "            if \"year\" not in df_i.columns:\n",
        "                df_i[\"year\"] = yr\n",
        "            else:\n",
        "                df_i[\"year\"] = pd.to_numeric(df_i[\"year\"], errors=\"coerce\").fillna(yr).astype(int)\n",
        "\n",
        "            df_i[\"__source_file\"] = os.path.basename(fp)\n",
        "\n",
        "            # -------- Dynamic Column Detection for the current file --------\n",
        "            customers_candidates = [\"customers_out\", \"customers\", \"cust_out\", \"customers_outage\", \"outage_customers\"]\n",
        "            fips_candidates      = [\"county_fips\", \"fips\", \"fips_code\", \"county_fips_code\", \"GEOID\"]\n",
        "            county_name_candidates = [\"county\", \"county_name\"]\n",
        "            lat_candidates       = [\"lat\", \"latitude\", \"y\", \"Lat\", \"Latitude\"]\n",
        "            lon_candidates       = [\"lon\", \"longitude\", \"x\", \"Lon\", \"Longitude\", \"long\"]\n",
        "            time_candidates      = [\"run_start_time\", \"start_time\", \"time\", \"timestamp\", \"datetime\"]\n",
        "\n",
        "            CUSTOMERS_COL   = next((c for c in customers_candidates if c in df_i.columns), None)\n",
        "            FIPS_COL        = next((c for c in fips_candidates if c in df_i.columns), None)\n",
        "            COUNTY_NAME_COL = next((c for c in county_name_candidates if c in df_i.columns), None)\n",
        "            LAT_COL         = next((c for c in lat_candidates if c in df_i.columns), None)\n",
        "            LON_COL         = next((c for c in lon_candidates if c in df_i.columns), None)\n",
        "            TIME_COL        = next((c for c in time_candidates if c in df_i.columns), None)\n",
        "\n",
        "            # Key sanity check: customers + year must exist\n",
        "            required_any = [CUSTOMERS_COL, \"year\"]\n",
        "            if any(col is None for col in required_any):\n",
        "                 print(f\"[WARN] Skipping file {fp}: Required columns missing. Detected columns: customers={CUSTOMERS_COL}, year={'year' in df_i.columns}\")\n",
        "                 file_report.append({\"year\": yr, \"path\": fp, \"original_rows\": original_rows, \"cleaned_rows\": 0, \"status\": f\"SKIPPED: Missing required columns\"})\n",
        "                 continue # Skip to next file\n",
        "\n",
        "            # -------- Data Cleaning for the current file --------\n",
        "            key_cols = [c for c in [FIPS_COL, CUSTOMERS_COL, \"year\", TIME_COL] if c is not None and c in df_i.columns]\n",
        "            for col in key_cols:\n",
        "                key_columns_used.add(col)\n",
        "\n",
        "            before = len(df_i)\n",
        "            df_cleaned = df_i.dropna(subset=key_cols).copy()\n",
        "            print(f\"[OK] Cleaned {fp}: Dropped rows with missing values in key columns ({before - len(df_cleaned):,} rows removed).\")\n",
        "\n",
        "            # Standardize FIPS if present and split into state and county\n",
        "            if FIPS_COL and FIPS_COL in df_cleaned.columns:\n",
        "                df_cleaned[FIPS_COL] = (\n",
        "                    df_cleaned[FIPS_COL]\n",
        "                    .astype(str)\n",
        "                    .str.replace(r\"\\.0$\", \"\", regex=True)\n",
        "                    .str.zfill(5)\n",
        "                )\n",
        "                # Split into State and County FIPS columns\n",
        "                df_cleaned['state_fips'] = df_cleaned[FIPS_COL].str[:2].str.zfill(2)\n",
        "                df_cleaned['county_fips'] = df_cleaned[FIPS_COL].str[2:].str.zfill(3)\n",
        "                print(f\"[OK] Cleaned {fp}: Formatted '{FIPS_COL}' as 5-digit strings and split into 'state_fips' and 'county_fips'.\")\n",
        "                # Drop the original FIPS column\n",
        "                df_cleaned.drop(columns=[FIPS_COL], inplace=True)\n",
        "                print(f\"[OK] Cleaned {fp}: Dropped original '{FIPS_COL}' column.\")\n",
        "\n",
        "\n",
        "            # Standardize time if present\n",
        "            if TIME_COL and TIME_COL in df_cleaned.columns:\n",
        "                before_time = len(df_cleaned)\n",
        "                df_cleaned[TIME_COL] = pd.to_datetime(df_cleaned[TIME_COL], errors=\"coerce\", utc=False)\n",
        "                df_cleaned.dropna(subset=[TIME_COL], inplace=True)\n",
        "                print(f\"[OK] Cleaned {fp}: Converted '{TIME_COL}' to datetime. Removed {before_time - len(df_cleaned):,} rows with invalid timestamps.\")\n",
        "\n",
        "            # Ensure customers numeric\n",
        "            if CUSTOMERS_COL and CUSTOMERS_COL in df_cleaned.columns:\n",
        "                 df_cleaned[CUSTOMERS_COL] = pd.to_numeric(df_cleaned[CUSTOMERS_COL], errors=\"coerce\").fillna(0)\n",
        "\n",
        "\n",
        "            # Final output columns for cleaned data (include new state and county FIPS if created)\n",
        "            output_cols = [col for col in [COUNTY_NAME_COL, CUSTOMERS_COL, \"year\", TIME_COL, \"__source_file\"] if col and col in df_cleaned.columns]\n",
        "            if 'state_fips' in df_cleaned.columns:\n",
        "                 output_cols.append('state_fips')\n",
        "            if 'county_fips' in df_cleaned.columns:\n",
        "                 output_cols.append('county_fips')\n",
        "\n",
        "            df_cleaned = df_cleaned[output_cols].copy()\n",
        "\n",
        "\n",
        "            # -------- Export Cleaned Data for the current year --------\n",
        "            cleaned_output_file_yearly = f\"/content/eaglei_outages_cleaned_{yr}.csv\"\n",
        "            export_dir = os.path.dirname(cleaned_output_file_yearly)\n",
        "            if export_dir and not os.path.exists(export_dir):\n",
        "                os.makedirs(export_dir)\n",
        "                print(f\"[INFO] Created directory: {export_dir}\")\n",
        "\n",
        "            try:\n",
        "                df_cleaned.to_csv(cleaned_output_file_yearly, index=False)\n",
        "                print(f\"[OK] Cleaned data exported for {yr} to: {cleaned_output_file_yearly}\")\n",
        "                file_report.append({\"year\": yr, \"path\": fp, \"original_rows\": original_rows, \"cleaned_rows\": len(df_cleaned), \"status\": \"EXPORTED\"})\n",
        "                total_cleaned_rows += len(df_cleaned)\n",
        "            except Exception as e:\n",
        "                print(f\"[WARN] Could not export cleaned data for {yr} to CSV: {e}\")\n",
        "                file_report.append({\"year\": yr, \"path\": fp, \"original_rows\": original_rows, \"cleaned_rows\": len(df_cleaned), \"status\": f\"EXPORT_ERROR: {e}\"})\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Error processing file {fp}: {e}\")\n",
        "            file_report.append({\"year\": yr, \"path\": fp, \"original_rows\": original_rows if 'original_rows' in locals() else 0, \"cleaned_rows\": 0, \"status\": f\"ERROR: {e}\"})\n",
        "    else:\n",
        "        missing_files.append(fp)\n",
        "        file_report.append({\"year\": yr, \"path\": fp, \"original_rows\": 0, \"cleaned_rows\": 0, \"status\": \"MISSING\"})\n",
        "\n",
        "# Print a quick report\n",
        "print(\"\\n=== File Load and Processing Report ===\")\n",
        "if file_report:\n",
        "    rep = pd.DataFrame(file_report).sort_values(\"year\")\n",
        "    print(rep.to_string(index=False))\n",
        "else:\n",
        "    print(\"No files were checked.\")\n",
        "\n",
        "if missing_files:\n",
        "    print(\"\\n[WARN] Missing files (not found on disk):\")\n",
        "    for m in missing_files:\n",
        "        print(f\" - {m}\")\n",
        "\n",
        "print(\"\\n=== Summary ===\")\n",
        "print(f\"Total original rows across all files: {total_original_rows:,}\")\n",
        "print(f\"Total rows remaining after cleaning: {total_cleaned_rows:,}\")\n",
        "print(f\"Key columns used for analysis: {', '.join(key_columns_used)}\")\n",
        "\n",
        "\n",
        "print(\"\\n✅ Complete: Data integration and cleaning for EAGLE-I outage data (2014–2023), saved as individual yearly files with state and county FIPS.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ce8cNwG8cKVT",
        "outputId": "9b441399-bcc1-4ee1-9855-17f80bbadf64"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Planned files for analysis (2014–2023):\n",
            " - 2014: /content/eaglei_outages_2014.csv\n",
            " - 2015: /content/eaglei_outages_2015.csv\n",
            " - 2016: /content/eaglei_outages_2016.csv\n",
            " - 2017: /content/eaglei_outages_2017.csv\n",
            " - 2018: /content/eaglei_outages_2018.csv\n",
            " - 2019: /content/eaglei_outages_2019.csv\n",
            " - 2020: /content/eaglei_outages_2020.csv\n",
            " - 2021: /content/eaglei_outages_2021.csv\n",
            " - 2022: /content/eaglei_outages_2022.csv\n",
            " - 2023: /content/eaglei_outages_2023.csv\n",
            "[OK] Cleaned /content/eaglei_outages_2014.csv: Dropped rows with missing values in key columns (0 rows removed).\n",
            "[OK] Cleaned /content/eaglei_outages_2014.csv: Formatted 'fips_code' as 5-digit strings and split into 'state_fips' and 'county_fips'.\n",
            "[OK] Cleaned /content/eaglei_outages_2014.csv: Dropped original 'fips_code' column.\n",
            "[OK] Cleaned /content/eaglei_outages_2014.csv: Converted 'run_start_time' to datetime. Removed 0 rows with invalid timestamps.\n",
            "[OK] Cleaned data exported for 2014 to: /content/eaglei_outages_cleaned_2014.csv\n",
            "[OK] Cleaned /content/eaglei_outages_2015.csv: Dropped rows with missing values in key columns (0 rows removed).\n",
            "[OK] Cleaned /content/eaglei_outages_2015.csv: Formatted 'fips_code' as 5-digit strings and split into 'state_fips' and 'county_fips'.\n",
            "[OK] Cleaned /content/eaglei_outages_2015.csv: Dropped original 'fips_code' column.\n",
            "[OK] Cleaned /content/eaglei_outages_2015.csv: Converted 'run_start_time' to datetime. Removed 0 rows with invalid timestamps.\n",
            "[OK] Cleaned data exported for 2015 to: /content/eaglei_outages_cleaned_2015.csv\n",
            "[OK] Cleaned /content/eaglei_outages_2016.csv: Dropped rows with missing values in key columns (0 rows removed).\n",
            "[OK] Cleaned /content/eaglei_outages_2016.csv: Formatted 'fips_code' as 5-digit strings and split into 'state_fips' and 'county_fips'.\n",
            "[OK] Cleaned /content/eaglei_outages_2016.csv: Dropped original 'fips_code' column.\n",
            "[OK] Cleaned /content/eaglei_outages_2016.csv: Converted 'run_start_time' to datetime. Removed 0 rows with invalid timestamps.\n",
            "[OK] Cleaned data exported for 2016 to: /content/eaglei_outages_cleaned_2016.csv\n",
            "[OK] Cleaned /content/eaglei_outages_2017.csv: Dropped rows with missing values in key columns (0 rows removed).\n",
            "[OK] Cleaned /content/eaglei_outages_2017.csv: Formatted 'fips_code' as 5-digit strings and split into 'state_fips' and 'county_fips'.\n",
            "[OK] Cleaned /content/eaglei_outages_2017.csv: Dropped original 'fips_code' column.\n",
            "[OK] Cleaned /content/eaglei_outages_2017.csv: Converted 'run_start_time' to datetime. Removed 0 rows with invalid timestamps.\n",
            "[OK] Cleaned data exported for 2017 to: /content/eaglei_outages_cleaned_2017.csv\n",
            "[OK] Cleaned /content/eaglei_outages_2018.csv: Dropped rows with missing values in key columns (5,971 rows removed).\n",
            "[OK] Cleaned /content/eaglei_outages_2018.csv: Converted 'run_start_time' to datetime. Removed 0 rows with invalid timestamps.\n",
            "[OK] Cleaned data exported for 2018 to: /content/eaglei_outages_cleaned_2018.csv\n",
            "[OK] Cleaned /content/eaglei_outages_2019.csv: Dropped rows with missing values in key columns (6,598 rows removed).\n",
            "[OK] Cleaned /content/eaglei_outages_2019.csv: Formatted 'fips_code' as 5-digit strings and split into 'state_fips' and 'county_fips'.\n",
            "[OK] Cleaned /content/eaglei_outages_2019.csv: Dropped original 'fips_code' column.\n",
            "[OK] Cleaned /content/eaglei_outages_2019.csv: Converted 'run_start_time' to datetime. Removed 0 rows with invalid timestamps.\n",
            "[OK] Cleaned data exported for 2019 to: /content/eaglei_outages_cleaned_2019.csv\n",
            "[OK] Cleaned /content/eaglei_outages_2020.csv: Dropped rows with missing values in key columns (446 rows removed).\n",
            "[OK] Cleaned /content/eaglei_outages_2020.csv: Formatted 'fips_code' as 5-digit strings and split into 'state_fips' and 'county_fips'.\n",
            "[OK] Cleaned /content/eaglei_outages_2020.csv: Dropped original 'fips_code' column.\n",
            "[OK] Cleaned /content/eaglei_outages_2020.csv: Converted 'run_start_time' to datetime. Removed 0 rows with invalid timestamps.\n",
            "[OK] Cleaned data exported for 2020 to: /content/eaglei_outages_cleaned_2020.csv\n",
            "[OK] Cleaned /content/eaglei_outages_2021.csv: Dropped rows with missing values in key columns (2,648 rows removed).\n",
            "[OK] Cleaned /content/eaglei_outages_2021.csv: Formatted 'fips_code' as 5-digit strings and split into 'state_fips' and 'county_fips'.\n",
            "[OK] Cleaned /content/eaglei_outages_2021.csv: Dropped original 'fips_code' column.\n",
            "[OK] Cleaned /content/eaglei_outages_2021.csv: Converted 'run_start_time' to datetime. Removed 0 rows with invalid timestamps.\n",
            "[OK] Cleaned data exported for 2021 to: /content/eaglei_outages_cleaned_2021.csv\n",
            "[OK] Cleaned /content/eaglei_outages_2022.csv: Dropped rows with missing values in key columns (0 rows removed).\n",
            "[OK] Cleaned /content/eaglei_outages_2022.csv: Formatted 'fips_code' as 5-digit strings and split into 'state_fips' and 'county_fips'.\n",
            "[OK] Cleaned /content/eaglei_outages_2022.csv: Dropped original 'fips_code' column.\n",
            "[OK] Cleaned /content/eaglei_outages_2022.csv: Converted 'run_start_time' to datetime. Removed 0 rows with invalid timestamps.\n",
            "[OK] Cleaned data exported for 2022 to: /content/eaglei_outages_cleaned_2022.csv\n",
            "[OK] Cleaned /content/eaglei_outages_2023.csv: Dropped rows with missing values in key columns (0 rows removed).\n",
            "[OK] Cleaned /content/eaglei_outages_2023.csv: Formatted 'fips_code' as 5-digit strings and split into 'state_fips' and 'county_fips'.\n",
            "[OK] Cleaned /content/eaglei_outages_2023.csv: Dropped original 'fips_code' column.\n",
            "[OK] Cleaned /content/eaglei_outages_2023.csv: Converted 'run_start_time' to datetime. Removed 0 rows with invalid timestamps.\n",
            "[OK] Cleaned data exported for 2023 to: /content/eaglei_outages_cleaned_2023.csv\n",
            "\n",
            "=== File Load and Processing Report ===\n",
            " year                             path  original_rows  cleaned_rows   status\n",
            " 2014 /content/eaglei_outages_2014.csv          25856         25856 EXPORTED\n",
            " 2015 /content/eaglei_outages_2015.csv          30748         30748 EXPORTED\n",
            " 2016 /content/eaglei_outages_2016.csv          24333         24333 EXPORTED\n",
            " 2017 /content/eaglei_outages_2017.csv          24578         24578 EXPORTED\n",
            " 2018 /content/eaglei_outages_2018.csv          17897         11926 EXPORTED\n",
            " 2019 /content/eaglei_outages_2019.csv          57796         51198 EXPORTED\n",
            " 2020 /content/eaglei_outages_2020.csv          40257         39811 EXPORTED\n",
            " 2021 /content/eaglei_outages_2021.csv          57876         55228 EXPORTED\n",
            " 2022 /content/eaglei_outages_2022.csv          56041         56041 EXPORTED\n",
            " 2023 /content/eaglei_outages_2023.csv          74538         74538 EXPORTED\n",
            "\n",
            "=== Summary ===\n",
            "Total original rows across all files: 409,920\n",
            "Total rows remaining after cleaning: 394,257\n",
            "Key columns used for analysis: run_start_time, fips_code, year, customers_out\n",
            "\n",
            "✅ Complete: Data integration and cleaning for EAGLE-I outage data (2014–2023), saved as individual yearly files with state and county FIPS.\n"
          ]
        }
      ]
    }
  ]
}