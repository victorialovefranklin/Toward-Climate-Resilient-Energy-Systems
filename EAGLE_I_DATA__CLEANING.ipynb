{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsO6c9TgZBiQGDjwVKUfL2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorialovefranklin/Toward-Climate-Resilient-Energy-Systems/blob/main/EAGLE_I_DATA__CLEANING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EAGLE-I (2014-2023) Data Integration & Cleaning**"
      ],
      "metadata": {
        "id": "KBvlHtwFcdcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task:** Load the 10 files Environment for Analysis of Geo-Located Energy Information (EAGLE-I) datasets across 2014 to 2023"
      ],
      "metadata": {
        "id": "dId64_MrWPcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# EAGLE-I (2014–2023) — Data Integration & Cleaning\n",
        "# ==============================\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# -------- Explicit file list (each year, 2014–2023) --------\n",
        "YEAR_TO_PATH = {\n",
        "    2014: \"/content/eaglei_outages_2014.csv\",\n",
        "    2015: \"/content/eaglei_outages_2015.csv\",\n",
        "    2016: \"/content/eaglei_outages_2016.csv\",\n",
        "    2017: \"/content/eaglei_outages_2017.csv\",\n",
        "    2018: \"/content/eaglei_outages_2018.csv\",\n",
        "    2019: \"/content/eaglei_outages_2019.csv\",\n",
        "    2020: \"/content/eaglei_outages_2020.csv\",\n",
        "    2021: \"/content/eaglei_outages_2021.csv\",\n",
        "    2022: \"/content/eaglei_outages_2022.csv\",\n",
        "    2023: \"/content/eaglei_outages_2023.csv\",\n",
        "}\n",
        "\n",
        "CLEANED_OUTPUT_FILE = \"/content/eaglei_outages_cleaned.csv\"\n",
        "\n",
        "print(\"Planned files for analysis (2014–2023):\")\n",
        "for y, p in YEAR_TO_PATH.items():\n",
        "    print(f\" - {y}: {p}\")\n",
        "\n",
        "# -------- Load and Concatenate Data --------\n",
        "frames = []\n",
        "file_report = []   # to summarize loads per file\n",
        "missing_files = []\n",
        "\n",
        "for yr, fp in YEAR_TO_PATH.items():\n",
        "    if os.path.exists(fp):\n",
        "        try:\n",
        "            df_i = pd.read_csv(fp, low_memory=False)\n",
        "            # force year column to exist and be int\n",
        "            if \"year\" not in df_i.columns:\n",
        "                df_i[\"year\"] = yr\n",
        "            else:\n",
        "                df_i[\"year\"] = pd.to_numeric(df_i[\"year\"], errors=\"coerce\").fillna(yr).astype(int)\n",
        "\n",
        "            df_i[\"__source_file\"] = os.path.basename(fp)\n",
        "            frames.append(df_i)\n",
        "            file_report.append({\"year\": yr, \"path\": fp, \"loaded_rows\": len(df_i), \"status\": \"OK\"})\n",
        "        except Exception as e:\n",
        "            file_report.append({\"year\": yr, \"path\": fp, \"loaded_rows\": 0, \"status\": f\"ERROR: {e}\"})\n",
        "    else:\n",
        "        missing_files.append(fp)\n",
        "        file_report.append({\"year\": yr, \"path\": fp, \"loaded_rows\": 0, \"status\": \"MISSING\"})\n",
        "\n",
        "# Print a quick report\n",
        "print(\"\\n=== File Load Report ===\")\n",
        "if file_report:\n",
        "    rep = pd.DataFrame(file_report).sort_values(\"year\")\n",
        "    print(rep.to_string(index=False))\n",
        "else:\n",
        "    print(\"No files were checked.\")\n",
        "\n",
        "if missing_files:\n",
        "    print(\"\\n[WARN] Missing files (not found on disk):\")\n",
        "    for m in missing_files:\n",
        "        print(f\" - {m}\")\n",
        "\n",
        "if not frames:\n",
        "    raise RuntimeError(\"No CSVs loaded for years 2014–2023. Check file paths or file availability.\")\n",
        "\n",
        "# Concatenate and enforce year range\n",
        "df = pd.concat(frames, ignore_index=True)\n",
        "df = df[(df[\"year\"] >= 2014) & (df[\"year\"] <= 2023)].copy()\n",
        "df.drop_duplicates(inplace=True)\n",
        "print(f\"\\n[OK] Loaded {len(df):,} rows across {len(frames)} files (2014–2023).\")\n",
        "\n",
        "# -------- Dynamic Column Detection --------\n",
        "customers_candidates = [\"customers_out\", \"customers\", \"cust_out\", \"customers_outage\", \"outage_customers\"]\n",
        "fips_candidates      = [\"county_fips\", \"fips\", \"fips_code\", \"county_fips_code\", \"GEOID\"]\n",
        "county_name_candidates = [\"county\", \"county_name\"]\n",
        "lat_candidates       = [\"lat\", \"latitude\", \"y\", \"Lat\", \"Latitude\"]\n",
        "lon_candidates       = [\"lon\", \"longitude\", \"x\", \"Lon\", \"Longitude\", \"long\"]\n",
        "time_candidates      = [\"run_start_time\", \"start_time\", \"time\", \"timestamp\", \"datetime\"]\n",
        "\n",
        "def first_existing(frame, candidates):\n",
        "    return next((c for c in candidates if c in frame.columns), None)\n",
        "\n",
        "CUSTOMERS_COL   = first_existing(df, customers_candidates)\n",
        "FIPS_COL        = first_existing(df, fips_candidates)\n",
        "COUNTY_NAME_COL = first_existing(df, county_name_candidates)\n",
        "LAT_COL         = first_existing(df, lat_candidates)\n",
        "LON_COL         = first_existing(df, lon_candidates)\n",
        "TIME_COL        = first_existing(df, time_candidates)\n",
        "\n",
        "# Key sanity check: customers + year must exist\n",
        "required_any = [CUSTOMERS_COL, \"year\"]\n",
        "if any(col is None for col in required_any):\n",
        "    raise ValueError(f\"Required columns missing. Detected columns: customers={CUSTOMERS_COL}, year={'year' in df.columns}\")\n",
        "\n",
        "# -------- Data Cleaning --------\n",
        "key_cols = [c for c in [FIPS_COL, CUSTOMERS_COL, \"year\", TIME_COL] if c is not None]\n",
        "before = len(df)\n",
        "df_cleaned = df.dropna(subset=key_cols).copy()\n",
        "print(f\"[OK] Dropped rows with missing values in key columns ({before - len(df_cleaned):,} rows removed).\")\n",
        "\n",
        "# Standardize FIPS if present\n",
        "if FIPS_COL in df_cleaned.columns:\n",
        "    df_cleaned[FIPS_COL] = (\n",
        "        df_cleaned[FIPS_COL]\n",
        "        .astype(str)\n",
        "        .str.replace(r\"\\.0$\", \"\", regex=True)\n",
        "        .str.zfill(5)\n",
        "    )\n",
        "    print(f\"[OK] Cleaned and formatted '{FIPS_COL}' as 5-digit strings.\")\n",
        "\n",
        "# Standardize time if present\n",
        "if TIME_COL in df_cleaned.columns:\n",
        "    df_cleaned[TIME_COL] = pd.to_datetime(df_cleaned[TIME_COL], errors=\"coerce\", utc=False)\n",
        "    before_time = len(df_cleaned)\n",
        "    df_cleaned.dropna(subset=[TIME_COL], inplace=True)\n",
        "    print(f\"[OK] Converted '{TIME_COL}' to datetime. Removed {before_time - len(df_cleaned):,} rows with invalid timestamps.\")\n",
        "\n",
        "# Ensure customers numeric\n",
        "df_cleaned[CUSTOMERS_COL] = pd.to_numeric(df_cleaned[CUSTOMERS_COL], errors=\"coerce\").fillna(0)\n",
        "\n",
        "# Final output columns for cleaned data\n",
        "output_cols = [col for col in [FIPS_COL, COUNTY_NAME_COL, CUSTOMERS_COL, \"year\", TIME_COL, \"__source_file\"] if col and col in df_cleaned.columns]\n",
        "df_cleaned = df_cleaned[output_cols].copy()\n",
        "\n",
        "# -------- Per-year counts to confirm inclusion --------\n",
        "year_counts = df_cleaned.groupby(\"year\")[CUSTOMERS_COL].size().rename(\"rows_loaded\").reset_index()\n",
        "print(\"\\n=== Rows Loaded Per Year (post-cleaning) ===\")\n",
        "print(year_counts.to_string(index=False))\n",
        "\n",
        "# -------- Export Cleaned Data --------\n",
        "try:\n",
        "    df_cleaned.to_csv(CLEANED_OUTPUT_FILE, index=False)\n",
        "    print(f\"\\n[OK] Cleaned data exported to: {CLEANED_OUTPUT_FILE}\")\n",
        "except Exception as e:\n",
        "    print(f\"[WARN] Could not export cleaned data to CSV: {e}\")\n",
        "\n",
        "print(\"\\n✅ Complete: Data integration and cleaning for EAGLE-I outage data (2014–2023).\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ce8cNwG8cKVT",
        "outputId": "1348e9be-cde8-4580-ab77-dc9d9f1f6ae6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Planned files for analysis (2014–2023):\n",
            " - 2014: /content/eaglei_outages_2014.csv\n",
            " - 2015: /content/eaglei_outages_2015.csv\n",
            " - 2016: /content/eaglei_outages_2016.csv\n",
            " - 2017: /content/eaglei_outages_2017.csv\n",
            " - 2018: /content/eaglei_outages_2018.csv\n",
            " - 2019: /content/eaglei_outages_2019.csv\n",
            " - 2020: /content/eaglei_outages_2020.csv\n",
            " - 2021: /content/eaglei_outages_2021.csv\n",
            " - 2022: /content/eaglei_outages_2022.csv\n",
            " - 2023: /content/eaglei_outages_2023.csv\n",
            "\n",
            "=== File Load Report ===\n",
            " year                             path  loaded_rows status\n",
            " 2014 /content/eaglei_outages_2014.csv        25856     OK\n",
            " 2015 /content/eaglei_outages_2015.csv        30748     OK\n",
            " 2016 /content/eaglei_outages_2016.csv        24333     OK\n",
            " 2017 /content/eaglei_outages_2017.csv        24578     OK\n",
            " 2018 /content/eaglei_outages_2018.csv        17897     OK\n",
            " 2019 /content/eaglei_outages_2019.csv        57796     OK\n",
            " 2020 /content/eaglei_outages_2020.csv        40257     OK\n",
            " 2021 /content/eaglei_outages_2021.csv        57876     OK\n",
            " 2022 /content/eaglei_outages_2022.csv        56041     OK\n",
            " 2023 /content/eaglei_outages_2023.csv        74538     OK\n",
            "\n",
            "[OK] Loaded 409,920 rows across 10 files (2014–2023).\n",
            "[OK] Dropped rows with missing values in key columns (27,589 rows removed).\n",
            "[OK] Cleaned and formatted 'fips_code' as 5-digit strings.\n",
            "[OK] Converted 'run_start_time' to datetime. Removed 0 rows with invalid timestamps.\n",
            "\n",
            "=== Rows Loaded Per Year (post-cleaning) ===\n",
            " year  rows_loaded\n",
            " 2014        25856\n",
            " 2015        30748\n",
            " 2016        24333\n",
            " 2017        24578\n",
            " 2019        51198\n",
            " 2020        39811\n",
            " 2021        55228\n",
            " 2022        56041\n",
            " 2023        74538\n",
            "\n",
            "[OK] Cleaned data exported to: /content/eaglei_outages_cleaned.csv\n",
            "\n",
            "✅ Complete: Data integration and cleaning for EAGLE-I outage data (2014–2023).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3165b710"
      },
      "source": [
        " # **Data Integration and Cleaning Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17977de6"
      },
      "source": [
        "\n",
        "\n",
        "**EAGLE-I Outage Data 2014-2023**\n",
        "\n",
        "10 files from the Environment for Analysis of Geo-Located Energy Information (EAGLE-I) datasets across 2014 to 2023 were manually filtered for the State of California. The 10 files were concatenated and merged into a single file named “eaglei_outages_cleaned.csv.” From those 10 files 409,920 rows were loaded successfully into Google Collab Python Notebook. The key candidate columns used such as 'customers out', fips_code, county, lat, lon, and run_start_time were analyzed to detect, identify, extract, and normalize columns fields across datasets to ensure consistent schema. As a result, 27,589 rows were removed, 382,331 rows remained.\n",
        "\n",
        "Row Counts per Year: A summary of the number of rows loaded per year after cleaning was provided, confirming the inclusion of data from 2014 to 2017 and 2019 to 2023. Data for 2018 is notably absent in the post-cleaning count, although it was reported as loaded. ORNL added 2024 outage data on 4/10/2025, however no customer outage coverage was reported for California.\n",
        "\n",
        "**Table 1: Pre-Cleaning (EAGLE-I 2014–2023)**\n",
        "\n",
        "| Year | File Path                | Rows Loaded |\n",
        "|------|--------------------------|-------------|\n",
        "| 2014 | eaglei_outages_2014.csv  | 25,856      |\n",
        "| 2015 | eaglei_outages_2015.csv  | 30,748      |\n",
        "| 2016 | eaglei_outages_2016.csv  | 24,333      |\n",
        "| 2017 | eaglei_outages_2017.csv  | 24,578      |\n",
        "| 2018 | eaglei_outages_2018.csv  | 17,897      |\n",
        "| 2019 | eaglei_outages_2019.csv  | 57,796      |\n",
        "| 2020 | eaglei_outages_2020.csv  | 40,257      |\n",
        "| 2021 | eaglei_outages_2021.csv  | 57,876      |\n",
        "| 2022 | eaglei_outages_2022.csv  | 56,041      |\n",
        "| 2023 | eaglei_outages_2023.csv  | 74,538      |\n",
        "\n",
        "**Table 2. Post Cleaning (EAGLE-I 2014–2023)**\n",
        "\n",
        "| Year | File Path                | Rows Loaded |\n",
        "|------|--------------------------|-------------|\n",
        "| 2014 | eaglei_outages_2014.csv  | 25,856      |\n",
        "| 2015 | eaglei_outages_2015.csv  | 30,748      |\n",
        "| 2016 | eaglei_outages_2016.csv  | 24,333      |\n",
        "| 2017 | eaglei_outages_2017.csv  | 24,578      |\n",
        "| 2019 | eaglei_outages_2018.csv  | 51,198      |\n",
        "| 2020 | eaglei_outages_2019.csv  | 39,811      |\n",
        "| 2021 | eaglei_outages_2020.csv  | 55,228      |\n",
        "| 2022 | eaglei_outages_2021.csv  | 56,041      |\n",
        "| 2023 | eaglei_outages_2022.csv  | 74,538      |\n",
        ""
      ]
    }
  ]
}